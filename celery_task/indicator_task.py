import uuid
from datetime import datetime
from typing import Optional

import pandas as pd
# import pandas_ta as ta  # Replaced with GPU implementation
from celery import Task
from loguru import logger
import io
import csv
from sqlalchemy import select, text, func
from sqlalchemy.dialects.postgresql import insert

from db_module.connect_sqlalchemy_engine import SyncSessionLocal
from models import OHLCV_MODELS, INDICATOR_MODELS, CryptoInfo
from models.indicator_progress import IndicatorProgress
from models.pipeline_state import (
    is_pipeline_active,
    set_component_error,
    PipelineComponent,
)
from . import celery_app
import redis
import os

# Redis Connection for Queue Management
REDIS_URL = os.getenv("CELERY_BROKER_URL", "redis://redis:6379/0")

def _get_redis_client():
    return redis.from_url(REDIS_URL)

def purge_indicators_queue():
    """
    'indicators' íë¥¼ ê°•ì œë¡œ ë¹„ì›ë‹ˆë‹¤.
    """
    try:
        r = _get_redis_client()
        # Celery uses the queue name as the Redis key for the list
        queue_name = "indicators"
        # Check length before deleting for logging
        length = r.llen(queue_name)
        if length > 0:
            r.delete(queue_name)
            logger.warning(f"[Queue] Purged 'indicators' queue (deleted {length} tasks)")
        else:
            logger.info("[Queue] 'indicators' queue is already empty")
    except Exception as e:
        logger.error(f"[Queue] Failed to purge 'indicators' queue: {e}")

def stop_all_indicator_tasks():
    """
    í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  ë³´ì¡°ì§€í‘œ ê´€ë ¨ íƒœìŠ¤í¬ë¥¼ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.
    """
    try:
        inspector = celery_app.control.inspect()
        active_tasks = inspector.active()
        
        if not active_tasks:
            logger.info("[Queue] No active workers found to stop tasks")
            return

        revoked_count = 0
        for worker_name, tasks in active_tasks.items():
            for task in tasks:
                # ë³´ì¡°ì§€í‘œ ê´€ë ¨ íƒœìŠ¤í¬ì¸ì§€ í™•ì¸ (ì´ë¦„ ë˜ëŠ” í)
                # task info: {'id': '...', 'name': '...', 'args': [...], ...}
                task_name = task.get("name", "")
                delivery_info = task.get("delivery_info", {})
                routing_key = delivery_info.get("routing_key", "")
                
                if task_name.startswith("indicator.") or routing_key == "indicators":
                    task_id = task["id"]
                    logger.warning(f"[Queue] Revoking task {task_id} ({task_name}) on {worker_name}")
                    celery_app.control.revoke(task_id, terminate=True, signal='SIGKILL')
                    revoked_count += 1
        
        if revoked_count > 0:
            logger.warning(f"[Queue] Revoked {revoked_count} active indicator tasks")
        else:
            logger.info("[Queue] No active indicator tasks found to revoke")
            
    except Exception as e:
        logger.error(f"[Queue] Failed to stop indicator tasks: {e}")

# [GPU Acceleration]
# DISABLED: cudf.pandas causes deadlock with high concurrency (24 threads)
# Using Numba CUDA for RSI only, CPU for EMA/SMA/BB
# try:
#     import cudf.pandas
#     cudf.pandas.install()
#     logger.info("[GPU] RAPIDS cuDF acceleration enabled!")
# except ImportError:
#     logger.warning("[GPU] RAPIDS cuDF not found, running on CPU.")
# except Exception as e:
#     logger.error(f"[GPU] Failed to enable cuDF: {e}")

logger.info("[GPU] Using Numba CUDA for RSI, CPU for other indicators (deadlock prevention)")


# Indicator ìœ ì§€ë³´ìˆ˜ ëŒ€ìƒ ì¸í„°ë²Œ(Backfill/RESTì™€ ë§ì¶¤)
# ì§§ì€ ì¸í„°ë²Œ(1m~30m)ì€ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
INTERVALS = ["1m", "3m", "5m", "15m", "30m", "1h", "4h", "1d", "1w", "1M"]


# =========================================================
#   ê³µí†µ: OHLCV ë¡œë”© + ë³´ì¡°ì§€í‘œ ê³„ì‚° + UPSERT
# =========================================================
def _load_ohlcv_ended_df(
    symbol: str, interval: str, limit: Optional[int] = None
) -> Optional[pd.DataFrame]:
    """
    trading_data.ohlcv_{interval} ì—ì„œ
    symbol, is_ended = true ì¸ ìº”ë“¤ì„ timestamp ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ DataFrameìœ¼ë¡œ ë¡œë“œ.
    limitê°€ ì§€ì •ë˜ë©´ 'ê°€ì¥ ìµœì‹  limitê°œ'ë§Œ ì‚¬ìš©.
    """
    OhlcvModel = OHLCV_MODELS.get(interval)
    if OhlcvModel is None:
        logger.error(f"[indicator] ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸í„°ë²Œ: {interval}")
        return None

    with SyncSessionLocal() as session:
        stmt = (
            select(
                OhlcvModel.timestamp,
                OhlcvModel.open,
                OhlcvModel.high,
                OhlcvModel.low,
                OhlcvModel.close,
                OhlcvModel.volume,
            )
            .where(OhlcvModel.symbol == symbol, OhlcvModel.is_ended.is_(True))
            .order_by(OhlcvModel.timestamp.desc())
        )

        if limit is not None:
            stmt = stmt.limit(limit)

        rows = session.execute(stmt).all()

    if not rows:
        return None

    # ìµœì‹  â†’ ê³¼ê±° ìˆœìœ¼ë¡œ ê°€ì ¸ì™”ìœ¼ë‹ˆ ë‹¤ì‹œ ì •ë ¬
    df = pd.DataFrame(
        [
            {
                "timestamp": r[0],
                "open": float(r[1]),
                "high": float(r[2]),
                "low": float(r[3]),
                "close": float(r[4]),
                "volume": float(r[5]),
            }
            for r in rows
        ]
    )

    df = df.sort_values("timestamp").set_index("timestamp")
    return df


def _load_ohlcv_incremental(
    symbol: str, interval: str, last_indicator_ts: Optional[datetime] = None
) -> Optional[pd.DataFrame]:
    """
    ì¦ë¶„ ê³„ì‚°ìš© OHLCV ë¡œë“œ.
    
    - last_indicator_tsê°€ Noneì´ë©´: ì „ì²´ ë°ì´í„° ë¡œë“œ (ìµœì´ˆ ê³„ì‚°)
    - last_indicator_tsê°€ ìˆìœ¼ë©´: ê·¸ ì´í›„ ë°ì´í„°ë§Œ ë¡œë“œ
      + ë‹¨, ë³´ì¡°ì§€í‘œ ê³„ì‚°ì„ ìœ„í•´ lookback ê¸°ê°„(100ê°œ) í¬í•¨
    
    Args:
        symbol: ì‹¬ë³¼
        interval: ì¸í„°ë²Œ
        last_indicator_ts: ë§ˆì§€ë§‰ìœ¼ë¡œ ê³„ì‚°ëœ ì§€í‘œì˜ timestamp
        
    Returns:
        OHLCV DataFrame (index=timestamp) ë˜ëŠ” None
    """
    OhlcvModel = OHLCV_MODELS.get(interval)
    if OhlcvModel is None:
        logger.error(f"[indicator] ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸í„°ë²Œ: {interval}")
        return None

    with SyncSessionLocal() as session:
        # ê¸°ë³¸ ì¿¼ë¦¬: is_ended=Trueì¸ ìº”ë“¤ë§Œ
        stmt = select(
            OhlcvModel.timestamp,
            OhlcvModel.open,
            OhlcvModel.high,
            OhlcvModel.low,
            OhlcvModel.close,
            OhlcvModel.volume,
        ).where(OhlcvModel.symbol == symbol, OhlcvModel.is_ended.is_(True))

        if last_indicator_ts is not None:
            # ì¦ë¶„ ê³„ì‚°: last_indicator_ts ì´í›„ ë°ì´í„°ë§Œ
            # ë‹¨, lookback ê¸°ê°„ì„ ìœ„í•´ 100ê°œ ì´ì „ë¶€í„° ë¡œë“œ
            
            # 1) last_indicator_ts ì´í›„ì˜ ëª¨ë“  ë°ì´í„°
            stmt_new = stmt.where(OhlcvModel.timestamp > last_indicator_ts)
            
            # 2) last_indicator_ts ì´ì „ 100ê°œ (warm-upìš©)
            stmt_lookback = (
                select(
                    OhlcvModel.timestamp,
                    OhlcvModel.open,
                    OhlcvModel.high,
                    OhlcvModel.low,
                    OhlcvModel.close,
                    OhlcvModel.volume,
                )
                .where(
                    OhlcvModel.symbol == symbol,
                    OhlcvModel.is_ended.is_(True),
                    OhlcvModel.timestamp <= last_indicator_ts,
                )
                .order_by(OhlcvModel.timestamp.desc())
                .limit(100)
            )
            
            # 3) ë‘ ì¿¼ë¦¬ ê²°ê³¼ í•©ì¹˜ê¸°
            rows_new = session.execute(stmt_new.order_by(OhlcvModel.timestamp.asc())).all()
            rows_lookback = session.execute(stmt_lookback).all()
            
            # lookbackì€ descë¡œ ê°€ì ¸ì™”ìœ¼ë‹ˆ reverse
            rows = list(reversed(rows_lookback)) + list(rows_new)
            
        else:
            # ìµœì´ˆ ê³„ì‚°: ì „ì²´ ë°ì´í„°
            stmt = stmt.order_by(OhlcvModel.timestamp.asc())
            rows = session.execute(stmt).all()

    if not rows:
        return None

    df = pd.DataFrame(
        [
            {
                "timestamp": r[0],
                "open": float(r[1]),
                "high": float(r[2]),
                "low": float(r[3]),
                "close": float(r[4]),
                "volume": float(r[5]),
            }
            for r in rows
        ]
    )

    df = df.sort_values("timestamp").set_index("timestamp")
    return df


def _get_last_indicator_timestamp(symbol: str, interval: str) -> Optional[datetime]:
    """
    indicators_{interval} í…Œì´ë¸”ì—ì„œ í•´ë‹¹ symbolì˜ ë§ˆì§€ë§‰ timestamp ì¡°íšŒ.
    
    Returns:
        ë§ˆì§€ë§‰ timestamp ë˜ëŠ” None (ë°ì´í„° ì—†ìœ¼ë©´)
    """
    IndicatorModel = INDICATOR_MODELS.get(interval)
    if IndicatorModel is None:
        return None
    
    with SyncSessionLocal() as session:
        result = (
            session.query(IndicatorModel.timestamp)
            .filter(IndicatorModel.symbol == symbol)
            .order_by(IndicatorModel.timestamp.desc())
            .limit(1)
            .first()
        )
    
    return result[0] if result else None


def _process_indicator_full(
    symbol: str,
    interval: str,
    run_id: Optional[str] = None,
) -> int:
    """
    ì „ì²´ ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ (Full Load & Calculation)
    
    ë©”ëª¨ë¦¬ ì œì•½ì„ ë¬´ì‹œí•˜ê³  ì†ë„ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•˜ì—¬,
    ëª¨ë“  OHLCV ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¡œë“œí•˜ê³  GPUë¡œ ì¼ê´„ ê³„ì‚°í•œ ë’¤ ì €ì¥í•©ë‹ˆë‹¤.
    """
    OhlcvModel = OHLCV_MODELS.get(interval)
    if not OhlcvModel:
        logger.error(f"[indicator_full] ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸í„°ë²Œ: {interval}")
        return 0
    
    # 1. ë¡œë“œí•  ë°ì´í„°ì˜ ì‹œì‘ ì‹œì  ê²°ì • (Gap Detection í¬í•¨)
    last_indicator_ts = _get_last_indicator_timestamp(symbol, interval)
    
    # Lookback count (100)
    lookback_count = 100
    
    with SyncSessionLocal() as session:
        # ì „ì²´ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ (ë””ë²„ê¹…ìš©)
        min_max = session.execute(
            select(func.min(OhlcvModel.timestamp), func.max(OhlcvModel.timestamp))
            .where(OhlcvModel.symbol == symbol, OhlcvModel.is_ended.is_(True))
        ).first()
        
        if not min_max or not min_max[0]:
            logger.info(f"[indicator_full] {symbol} {interval}: OHLCV ë°ì´í„° ì—†ìŒ")
            return 0
            
        min_ts, max_ts = min_max
        
        # ì¿¼ë¦¬ êµ¬ì„±
        stmt = select(
            OhlcvModel.timestamp,
            OhlcvModel.open,
            OhlcvModel.high,
            OhlcvModel.low,
            OhlcvModel.close,
            OhlcvModel.volume,
        ).where(
            OhlcvModel.symbol == symbol,
            OhlcvModel.is_ended.is_(True)
        ).order_by(OhlcvModel.timestamp.asc())
        
        if last_indicator_ts:
            # ì¦ë¶„ ê³„ì‚°: last_indicator_ts ì´í›„ ë°ì´í„° + Lookback
            # Lookbackì„ ìœ„í•´ last_indicator_ts ì´ì „ 100ê°œë„ ê°€ì ¸ì™€ì•¼ í•¨.
            # í•˜ì§€ë§Œ ì¿¼ë¦¬ê°€ ë³µì¡í•´ì§€ë¯€ë¡œ, ê°„ë‹¨íˆ "ì „ì²´ ë¡œë“œ" ì „ëµì„ ì‚¬ìš©í•˜ê±°ë‚˜
            # ì•„ë‹ˆë©´ ìœ„ì—ì„œ êµ¬í˜„í–ˆë˜ _load_ohlcv_incremental ë¡œì§ì„ ì°¨ìš©.
            # ì‚¬ìš©ì ìš”ì²­ì´ "ì „ì²´ ë¡œë“œ" ë‰˜ì•™ìŠ¤ì˜€ì§€ë§Œ, 
            # ì´ë¯¸ ê³„ì‚°ëœ ê³¼ê±° ë°ì´í„°ê¹Œì§€ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” ê±´ ë‚­ë¹„ì¼ ìˆ˜ ìˆìŒ.
            # ê·¸ëŸ¬ë‚˜ "Gap Detection"ì„ í™•ì‹¤íˆ í•˜ë ¤ë©´ last_indicator_ts ì´í›„ë¶€í„°ê°€ ë§ìŒ.
            
            # ì—¬ê¸°ì„œëŠ” íš¨ìœ¨ì„±ì„ ìœ„í•´ _load_ohlcv_incremental ì‚¬ìš© (Lookback í¬í•¨ ë¡œë“œ)
            # ë‹¨, í•¨ìˆ˜ ì´ë¦„ì´ _process_indicator_full ì´ë¯€ë¡œ "ë°°ì¹˜ ì—†ì´ í•œë°©ì—"ê°€ í•µì‹¬.
            pass
            
    # _load_ohlcv_incremental í•¨ìˆ˜ê°€ ì´ë¯¸ Lookback í¬í•¨ ë¡œë“œë¥¼ ì˜ êµ¬í˜„í•˜ê³  ìˆìŒ.
    df = _load_ohlcv_incremental(symbol, interval, last_indicator_ts)
    
    if df is None or df.empty:
        logger.info(f"[indicator_full] {symbol} {interval}: ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ")
        return 0
        
    # ì €ì¥í•´ì•¼ í•  ì‹¤ì œ ë°ì´í„°ì˜ ì‹œì‘ ì‹œì  (Lookback ì œì™¸)
    if last_indicator_ts:
        save_start_ts = last_indicator_ts  # last_indicator_ts ë‹¤ìŒë¶€í„° ì €ì¥í•´ì•¼ í•¨ (ì¤‘ë³µ ë°©ì§€ ë¡œì§ í•„ìš”)
        # _load_ohlcv_incrementalì€ last_indicator_ts < timestamp ì¸ ë°ì´í„° + lookbackì„ ê°€ì ¸ì˜´.
        # ë”°ë¼ì„œ dfì˜ ë°ì´í„° ì¤‘ last_indicator_tsë³´ë‹¤ í° ê²ƒë§Œ ì €ì¥í•˜ë©´ ë¨.
        # ê·¼ë° _load_ohlcv_incremental ë¡œì§ìƒ lookbackì€ <= last_indicator_ts ì„.
        # ê·¸ëŸ¬ë¯€ë¡œ save_start_tsëŠ” last_indicator_tsë³´ë‹¤ ì»¤ì•¼ í•¨.
        pass
    else:
        save_start_ts = df.index[0] # ìµœì´ˆ ê³„ì‚° ì‹œ ì „ì²´ ì €ì¥
        
    logger.info(
        f"[indicator_full] {symbol} {interval}: "
        f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)} rows). GPU ê³„ì‚° ì‹œì‘..."
    )
    
    # 2. GPU ì¼ê´„ ê³„ì‚°
    df_ind = _compute_indicators(df)
    
    if df_ind.empty:
        logger.warning(f"[indicator_full] {symbol} {interval}: ì§€í‘œ ê³„ì‚° ê²°ê³¼ ì—†ìŒ")
        return 0
        
    # 3. ì €ì¥ ëŒ€ìƒ í•„í„°ë§ (Lookback ì œì™¸)
    if last_indicator_ts:
        df_to_save = df_ind[df_ind.index > last_indicator_ts]
    else:
        df_to_save = df_ind
        
    if df_to_save.empty:
        logger.info(f"[indicator_full] {symbol} {interval}: ì €ì¥í•  ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")
        return 0
        
    # 4. ê³ ì† ì €ì¥ (COPY)
    saved_count = _bulk_upsert_indicators_via_copy(symbol, interval, df_to_save)
    
    logger.info(
        f"[indicator_full] {symbol} {interval}: "
        f"ì²˜ë¦¬ ì™„ë£Œ (ì´ {saved_count}ê°œ ì €ì¥)"
    )
    
    return saved_count



def _compute_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    OHLCV DataFrame(index=timestamp)ì— ë³´ì¡°ì§€í‘œ ì»¬ëŸ¼ë“¤ì„ ê³„ì‚°í•´ì„œ ë¦¬í„´.
    (rsi_14, ema_7, ema_21, ema_99, macd, macd_signal, macd_hist, bb_*, volume_20)
    
    GPU-accelerated version using Numba CUDA
    """
    if df.empty:
        return df

    # Import GPU indicators
    from .gpu_indicators import compute_indicators_gpu
    
    try:
        # Use GPU accelerated computation
        df_result = compute_indicators_gpu(df)
        
        # Extract only the columns we need
        wanted_cols = [
            "rsi_14",
            "ema_7",
            "ema_21",
            "ema_99",
            "macd",
            "macd_signal",
            "macd_hist",
            "bb_upper",
            "bb_middle",
            "bb_lower",
            "volume_20",
        ]
        
        # Ensure all columns exist
        for col in wanted_cols:
            if col not in df_result.columns:
                logger.warning(
                    f"[indicator] missing column '{col}' in computed df, filling with NaN"
                )
                df_result[col] = pd.NA

        # ema_99ëŠ” 99ê°œì˜ ìº”ë“¤ì´ í•„ìš”í•˜ë¯€ë¡œ 1M ê°™ì€ ê²½ìš° ë°ì´í„°ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŒ
        # ema_99ë¥¼ ì œì™¸í•œ ì»¬ëŸ¼ë“¤ë§Œ dropna() ì ìš©
        required_cols = [
            "rsi_14",
            "ema_7",
            "ema_21",
            # "ema_99",  # ì œì™¸: nullable
            "macd",
            "macd_signal",
            "macd_hist",
            "bb_upper",
            "bb_middle",
            "bb_lower",
            "volume_20",
        ]
        
        # required_colsì— ëŒ€í•´ì„œë§Œ dropna ìˆ˜í–‰
        df_ind = df_result[wanted_cols].dropna(subset=required_cols)
        return df_ind
        
    except Exception as e:
        logger.error(f"[GPU Indicator] Failed to compute GPU indicators: {e}")
        logger.error(f"[GPU Indicator] Falling back to empty result")
        # Return empty DataFrame with expected columns
        result = pd.DataFrame(index=df.index)
        for col in wanted_cols:
            result[col] = pd.NA
        return result


def _upsert_indicators(
    symbol: str, interval: str, df_ind: pd.DataFrame, only_last: bool = False
) -> int:
    """
    ê³„ì‚°ëœ ë³´ì¡°ì§€í‘œ df_ind(index=timestamp)ë¥¼
    trading_data.indicators_{interval}ì— UPSERT.

    only_last = Trueë©´ ë§ˆì§€ë§‰ 1ê°œë§Œ upsert (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ìš©).
    ë¦¬í„´: upsertëœ row ê°œìˆ˜
    """
    IndicatorModel = INDICATOR_MODELS.get(interval)
    if IndicatorModel is None:
        logger.error(
            f"[indicator] ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸í„°ë²Œ(IndicatorModel ì—†ìŒ): {interval}"
        )
        return 0

    if df_ind.empty:
        return 0

    if only_last:
        df_ind = df_ind.tail(1)

    df_reset = df_ind.reset_index()  # timestamp ì»¬ëŸ¼ìœ¼ë¡œ ë³µêµ¬
    records = []
    for _, row in df_reset.iterrows():
        records.append(
            {
                "symbol": symbol,
                "timestamp": row["timestamp"],
                "rsi_14": float(row["rsi_14"]),
                "ema_7": float(row["ema_7"]),
                "ema_21": float(row["ema_21"]),
                "ema_99": float(row["ema_99"]) if not pd.isna(row["ema_99"]) else None,
                "macd": float(row["macd"]),
                "macd_signal": float(row["macd_signal"]),
                "macd_hist": float(row["macd_hist"]),
                "bb_upper": float(row["bb_upper"]),
                "bb_middle": float(row["bb_middle"]),
                "bb_lower": float(row["bb_lower"]),
                "volume_20": float(row["volume_20"]),
            }
        )

    if not records:
        return 0

    with SyncSessionLocal() as session, session.begin():
        stmt = insert(IndicatorModel).values(records)
        keys = records[0].keys()

        update_cols = {
            k: getattr(stmt.excluded, k)
            for k in keys
            if k not in ("symbol", "timestamp")
        }

        stmt = stmt.on_conflict_do_update(
            index_elements=["symbol", "timestamp"],
            set_=update_cols,
        )
        session.execute(stmt)

    return len(records)


def _bulk_upsert_indicators_via_copy(
    symbol: str, interval: str, df_ind: pd.DataFrame
) -> int:
    """
    PostgreSQL COPY ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëŸ‰ì˜ ë³´ì¡°ì§€í‘œ ë°ì´í„°ë¥¼ ê³ ì†ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Process:
    1. DataFrameì„ ë©”ëª¨ë¦¬ ìƒì˜ CSVë¡œ ë³€í™˜
    2. Temp Table ìƒì„±
    3. COPY ëª…ë ¹ì–´ë¡œ CSV ë°ì´í„°ë¥¼ Temp Tableì— ë¡œë“œ
    4. INSERT INTO ... SELECT ... ON CONFLICT ë¡œ Target Tableì— ë³‘í•©
    
    Returns:
        ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
    """
    if df_ind.empty:
        return 0

    IndicatorModel = INDICATOR_MODELS.get(interval)
    if IndicatorModel is None:
        logger.error(f"[indicator] ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸í„°ë²Œ: {interval}")
        return 0

    # 1. Prepare Data
    df_reset = df_ind.reset_index()
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ ë° ìˆœì„œ ë³´ì¥ (symbol í¬í•¨)
    columns = [
        "symbol", "timestamp", 
        "rsi_14", "ema_7", "ema_21", "ema_99", 
        "macd", "macd_signal", "macd_hist", 
        "bb_upper", "bb_middle", "bb_lower", 
        "volume_20"
    ]
    
    # symbol ì»¬ëŸ¼ ì¶”ê°€
    df_reset["symbol"] = symbol
    
    # ğŸš€ Refactored Chunked Implementation with Retry Logic
    # Instead of one giant COPY, we split the dataframe and perform multiple COPY -> INSERT cycles.
    # Added retry logic for DeadlockDetected errors.
    
    from psycopg2.errors import DeadlockDetected
    import time
    import random

    CHUNK_SIZE = 2000 # Reduced from 10000 to 2000 to minimize lock contention
    total_rows = len(df_reset)
    saved_count = 0
    
    # If data is small, just do it once
    if total_rows <= CHUNK_SIZE:
        chunks = [df_reset]
    else:
        chunks = [df_reset[i:i + CHUNK_SIZE] for i in range(0, total_rows, CHUNK_SIZE)]
        logger.info(f"[indicator.copy] Splitting {total_rows} rows into {len(chunks)} chunks for {symbol} {interval}")

    # columnsëŠ” ìœ„ì—ì„œ ì •ì˜í•œ ê²ƒì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (symbol í¬í•¨)
    cols_str = ", ".join(columns)
    update_set = ", ".join([
        f"{col} = EXCLUDED.{col}" 
        for col in columns 
        if col not in ("symbol", "timestamp")
    ])
    
    table_name = IndicatorModel.__tablename__
    schema_name = IndicatorModel.__table__.schema
    full_table_name = f"{schema_name}.{table_name}"

    with SyncSessionLocal() as session:
        # connection = session.connection() # Don't get it here
        # dbapi_conn = connection.connection # Don't get it here
        
        try:
            for i, chunk in enumerate(chunks):
                retries = 3
                while retries > 0:
                    # Ensure connection is active and get raw connection for EACH retry/iteration
                    # session.connection() ensures a transaction is active and connection is checked out
                    dbapi_conn = session.connection().connection
                    cursor = dbapi_conn.cursor()
                    try:
                        # Temp table per chunk
                        temp_table_name = f"temp_{table_name}_{uuid.uuid4().hex[:8]}".lower()
                        
                        # chunkëŠ” ì´ë¯¸ df_resetì˜ sliceì´ë¯€ë¡œ symbol, timestamp ì»¬ëŸ¼ì´ ì¡´ì¬í•¨.
                        # to_csv í˜¸ì¶œ ì‹œ index=Falseë¡œ ì„¤ì •í•´ì•¼ í•¨ (timestampê°€ ì»¬ëŸ¼ìœ¼ë¡œ ì¡´ì¬í•˜ë¯€ë¡œ)
                        
                        csv_buffer = io.StringIO()
                        chunk.to_csv(
                            csv_buffer,
                            sep='\t',
                            index=False, 
                            header=False,
                            date_format='%Y-%m-%d %H:%M:%S',
                            columns=columns,
                            na_rep='\\N'
                        )
                        csv_buffer.seek(0)
                        
                        # Create Temp Table
                        cursor.execute(f"""
                            CREATE TEMP TABLE {temp_table_name} 
                            (LIKE {full_table_name} INCLUDING DEFAULTS)
                            ON COMMIT DROP;
                        """)
                        
                        # COPY to Temp
                        cursor.copy_from(
                            csv_buffer, 
                            temp_table_name, 
                            sep='\t', 
                            null='\\N',
                            columns=columns
                        )
                        
                        # INSERT to Target
                        query = f"""
                            INSERT INTO {full_table_name} ({cols_str})
                            SELECT {cols_str}
                            FROM {temp_table_name}
                            ON CONFLICT (symbol, timestamp) 
                            DO UPDATE SET {update_set};
                        """
                        cursor.execute(query)
                        saved_count += cursor.rowcount
                        
                        # Drop temp table explicitly
                        cursor.execute(f"DROP TABLE IF EXISTS {temp_table_name}")
                        
                        # Commit every chunk
                        session.commit()
                        
                        # Success, break retry loop
                        break
                        
                    except DeadlockDetected:
                        session.rollback() # Rollback current transaction
                        retries -= 1
                        if retries == 0:
                            logger.error(f"[indicator.copy] Deadlock detected and retries exhausted for chunk {i} of {symbol} {interval}")
                            raise
                        
                        sleep_time = random.uniform(0.1, 0.5) * (4 - retries) # Exponential backoff-ish
                        logger.warning(f"[indicator.copy] Deadlock detected for chunk {i}, retrying in {sleep_time:.2f}s... ({retries} left)")
                        time.sleep(sleep_time)
                        
                    except Exception as e:
                        # Other errors, rollback and re-raise
                        session.rollback()
                        logger.error(f"[indicator.copy] Failed to bulk upsert (chunked): {e}")
                        raise
                    finally:
                        cursor.close()
                
            return saved_count
            
        except Exception as e:
            raise


# =========================================================
#   indicator_progress UPSERT (ìœ ì§€ë³´ìˆ˜ ì—”ì§„ UIìš©)
# =========================================================
def upsert_indicator_progress(
    run_id: str,
    symbol: str,
    interval: str,
    state: str,
    pct_time: float = 0.0,
    last_ts: Optional[datetime] = None,
    error: Optional[str] = None,
):
    """trading_data.indicator_progress UPSERT."""
    if not run_id:
        return

    with SyncSessionLocal() as session, session.begin():
        stmt = (
            insert(IndicatorProgress)
            .values(
                run_id=run_id,
                symbol=symbol,
                interval=interval,
                state=state,
                pct_time=pct_time,
                last_candle_ts=last_ts,
                last_error=error,
            )
            .on_conflict_do_update(
                index_elements=["run_id", "symbol", "interval"],
                set_={
                    "state": state,
                    "pct_time": pct_time,
                    "last_candle_ts": last_ts,
                    "last_error": error,
                    "updated_at": text("now()"),
                },
            )
        )
        session.execute(stmt)


# =====================
# â‘  ìµœì´ˆ ëŒ€ëŸ‰ ê³„ì‚°(ì‹¬ë³¼/ì¸í„°ë²Œ ë‹¨ìœ„) â€” í•„ìš”ì‹œ ì‚¬ìš©
# =====================
@celery_app.task(bind=True, name="indicator.bulk_init_indicators_symbol_interval", queue='indicators')
def bulk_init_indicators_symbol_interval(
    self: Task, symbol: str, interval: str
) -> dict:
    """
    ìµœì´ˆ ëŒ€ëŸ‰ ë³´ì¡°ì§€í‘œ ê³„ì‚° íƒœìŠ¤í¬.
    - í•´ë‹¹ symbol, intervalì— ëŒ€í•´ is_ended = true ì¸ ëª¨ë“  ìº”ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ
      ë³´ì¡°ì§€í‘œë¥¼ ì „ë¶€ ë‹¤ì‹œ ê³„ì‚°í•´ì„œ indicators_* í…Œì´ë¸”ì— upsert.
    - íŒŒì´í”„ë¼ì¸ì´ OFF ìƒíƒœë©´ ë°”ë¡œ SKIP ë¦¬í„´.
    """
    # íŒŒì´í”„ë¼ì¸ OFFë©´ ì‘ì—… ìŠ¤í‚µ
    if not is_pipeline_active():
        logger.info(
            f"[indicator.bulk_init] pipeline inactive -> skip ({symbol} {interval})"
        )
        return {
            "status": "SKIP_PIPELINE_OFF",
            "symbol": symbol,
            "interval": interval,
        }

    logger.info(f"[indicator.bulk_init] start: {symbol} {interval}")

    try:
        df_ohlcv = _load_ohlcv_ended_df(symbol, interval, limit=None)
        if df_ohlcv is None or df_ohlcv.empty:
            logger.warning(
                f"[indicator.bulk_init] no OHLCV data for {symbol} {interval}"
            )
            return {
                "status": "NO_DATA",
                "symbol": symbol,
                "interval": interval,
            }

        df_ind = _compute_indicators(df_ohlcv)
        if df_ind.empty:
            logger.warning(
                f"[indicator.bulk_init] indicators empty for {symbol} {interval}"
            )
            return {
                "status": "EMPTY_INDICATORS",
                "symbol": symbol,
                "interval": interval,
            }

        count = _upsert_indicators(symbol, interval, df_ind, only_last=False)

        logger.info(f"[indicator.bulk_init] done {symbol} {interval} (rows={count})")
        return {
            "status": "COMPLETE",
            "symbol": symbol,
            "interval": interval,
            "rows": count,
        }

    except Exception as e:
        msg = f"bulk_init failed for {symbol} {interval}: {type(e).__name__}: {e}"
        logger.error(f"[indicator.bulk_init] {msg}")
        # Indicator ì—”ì§„ ì—ëŸ¬ ê¸°ë¡ (id=5)
        try:
            set_component_error(PipelineComponent.INDICATOR, msg)
        except Exception:
            logger.exception("[indicator.bulk_init] failed to save last_error")
        # Celery ìª½ì—ë„ ì‹¤íŒ¨ë¡œ ë‚¨ê¸°ê¸°
        raise


# =====================
# â‘¡ ì‹¤ì‹œê°„ìš© per-symbol íƒœìŠ¤í¬ (ì›¹ì†Œì¼“ì—ì„œ ì‚¬ìš©)
# =====================
@celery_app.task(bind=True, name="indicator.update_last_indicator_for_symbol_interval")
def update_last_indicator_for_symbol_interval(
    self: Task, symbol: str, interval: str
) -> dict:
    """
    ì‹¤ì‹œê°„ ìœ ì§€ë³´ìˆ˜ìš© íƒœìŠ¤í¬.
    - ìµœê·¼ Nê°œ OHLCVë§Œ ë¶ˆëŸ¬ì™€ì„œ ì§€í‘œ ê³„ì‚°
    - ë§ˆì§€ë§‰ 1ê°œë§Œ indicators_*ì— upsert
    - websocket_taskì—ì„œ 'ìº”ë“¤ì´ ë‹«í˜”ë‹¤(is_ended=True)' ì´ë²¤íŠ¸ ë°œìƒ ì‹œ í˜¸ì¶œí•˜ëŠ” ìš©ë„
    - íŒŒì´í”„ë¼ì¸ì´ OFFë©´ ë°”ë¡œ SKIP.
    """
    if not is_pipeline_active():
        logger.info(
            f"[indicator.update_last] pipeline inactive -> skip ({symbol} {interval})"
        )
        return {
            "status": "SKIP_PIPELINE_OFF",
            "symbol": symbol,
            "interval": interval,
        }

    logger.info(f"[indicator.update_last] start: {symbol} {interval}")

    try:
        df_ohlcv = _load_ohlcv_ended_df(symbol, interval, limit=200)
        if df_ohlcv is None or df_ohlcv.empty:
            logger.warning(
                f"[indicator.update_last] no OHLCV data for {symbol} {interval}"
            )
            return {
                "status": "NO_DATA",
                "symbol": symbol,
                "interval": interval,
            }

        df_ind = _compute_indicators(df_ohlcv)
        if df_ind.empty:
            logger.warning(
                f"[indicator.update_last] indicators empty for {symbol} {interval}"
            )
            return {
                "status": "EMPTY_INDICATORS",
                "symbol": symbol,
                "interval": interval,
            }

        count = _upsert_indicators(symbol, interval, df_ind, only_last=True)
        last_ts: datetime = df_ind.index[-1]

        logger.info(
            f"[indicator.update_last] done {symbol} {interval} "
            f"(rows={count}, last_ts={last_ts.isoformat()})"
        )

        return {
            "status": "COMPLETE",
            "symbol": symbol,
            "interval": interval,
            "rows": count,
            "last_timestamp": last_ts.isoformat(),
        }

    except Exception as e:
        msg = f"update_last failed for {symbol} {interval}: {type(e).__name__}: {e}"
        logger.error(f"[indicator.update_last] {msg}")
        try:
            set_component_error(PipelineComponent.INDICATOR, msg)
        except Exception:
            logger.exception("[indicator.update_last] failed to save last_error")
        raise


# =====================
# â‘¢ íŒŒì´í”„ë¼ì¸ìš© Indicator ìœ ì§€ë³´ìˆ˜ ì—”ì§„
#     (ëª¨ë“  ì‹¬ë³¼Ã—ì¸í„°ë²Œì„ í•œ ë²ˆì— ëŒë¦¬ê³  ì§„í–‰í˜„í™© ì €ì¥)
# =====================
@celery_app.task(name="indicator.run_indicator_maintenance")
def run_indicator_maintenance() -> list:
    """
    íŒŒì´í”„ë¼ì¸ Maintenance ì‚¬ì´í´ì—ì„œ í˜¸ì¶œë˜ëŠ” ë³´ì¡°ì§€í‘œ ì—”ì§„.
    - ëª¨ë“  ì‹¬ë³¼ Ã— INTERVALS ì— ëŒ€í•´:
        * OHLCV(is_ended=True) ë¡œë¶€í„° ë³´ì¡°ì§€í‘œ ì „ë¶€ ì¬ê³„ì‚°
        * indicators_{interval} ì— upsert
        * indicator_progress ì— PENDING/PROGRESS/SUCCESS/FAILURE ê¸°ë¡
    - ì§„í–‰ë¥  pct_time ì€ ê°„ë‹¨íˆ
        * ì‘ì—… ì‹œì‘ ì‹œ 0
        * ê³„ì‚°/ì €ì¥ ì™„ë£Œ ì‹œ 100 ìœ¼ë¡œë§Œ ì‚¬ìš© (ì„¸ë°€í•œ %ëŠ” ìƒëµ)
    """
    logger.info("[Indicator] ìœ ì§€ë³´ìˆ˜ ì—”ì§„ ì‹œì‘")

    if not is_pipeline_active():
        logger.info("[Indicator] pipeline inactive â†’ ì¢…ë£Œ")
        return {"status": "INACTIVE"}

    run_id = f"ind-{uuid.uuid4().hex}"
    logger.info(f"[Indicator] run_id={run_id}")

    # 0) í ì´ˆê¸°í™” (ê¸°ì¡´ì— ìŒ“ì¸ ì‘ì—… ì‚­ì œ)
    # ìƒˆë¡œìš´ ìœ ì§€ë³´ìˆ˜ ì‚¬ì´í´ì´ ì‹œì‘ë˜ë¯€ë¡œ, ì´ì „ì˜ ì”ì—¬ ì‘ì—…ì€ ì˜ë¯¸ê°€ ì—†ìŒ
    purge_indicators_queue()

    # 1) ì‹¬ë³¼ ëª©ë¡ ì¡°íšŒ
    with SyncSessionLocal() as session:
        symbols = (
            session.query(CryptoInfo.symbol, CryptoInfo.pair)
            .filter(CryptoInfo.pair.isnot(None))
            .all()
        )

    # 2) ëª¨ë“  ì‹¬ë³¼Ã—ì¸í„°ë²Œì— PENDING dummy row ìƒì„±
    with SyncSessionLocal() as session, session.begin():
        for sym, _ in symbols:
            for interval in INTERVALS:
                stmt = (
                    insert(IndicatorProgress)
                    .values(
                        run_id=run_id,
                        symbol=sym,
                        interval=interval,
                        state="PENDING",
                        pct_time=0.0,
                        last_candle_ts=None,
                        last_error=None,
                    )
                    .on_conflict_do_nothing()
                )
                session.execute(stmt)

    # 3) ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ Task ê·¸ë£¹ ìƒì„± (ìš°ì„ ìˆœìœ„ ì ìš©)
    from celery import group
    
    # Interval ìš°ì„ ìˆœìœ„ ë§µ (í° ì¸í„°ë²Œì´ ë†’ì€ ìš°ì„ ìˆœìœ„)
    # í° ì¸í„°ë²Œ(1M, 1w)ì€ ë°ì´í„° ì ì–´ì„œ ë¹ ë¦„ â†’ ë¨¼ì € ì²˜ë¦¬í•˜ì—¬ ë¹ ë¥¸ í”¼ë“œë°±
    INTERVAL_PRIORITY = {
        '1M': 10,   # ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„
        '1w': 9,
        '1d': 8,
        '4h': 7,
        '1h': 6,
        '30m': 5,
        '15m': 4,
        '5m': 3,
        '3m': 2,
        '1m': 1,    # ê°€ì¥ ë‚®ì€ ìš°ì„ ìˆœìœ„ (ë°ì´í„° ë§ì•„ì„œ ëŠë¦¼)
    }
    
    tasks = []
    for sym, _pair in symbols:
        for interval in INTERVALS:
            priority = INTERVAL_PRIORITY.get(interval, 5)  # ê¸°ë³¸ê°’ 5
            
            # apply_asyncë¡œ priority ì§€ì • -> Signatureë¡œ ë³€ê²½
            # Chordì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Signature ê°ì²´ë¥¼ ë°˜í™˜í•´ì•¼ í•¨
            sig = maintain_symbol_interval.s(sym, interval).set(
                queue='indicators', 
                priority=priority
            )
            tasks.append(sig)
    
    if not tasks:
        logger.info("[Indicator] ì²˜ë¦¬í•  íƒœìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"status": "NO_TASKS"}

    logger.info(f"[Indicator] {len(tasks)}ê°œì˜ íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘ (ìš°ì„ ìˆœìœ„ ì ìš©)")
    # íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (Callerê°€ chordë¡œ ì‹¤í–‰í•˜ë„ë¡)
    return tasks


@celery_app.task(bind=True, name="indicator.maintain_symbol_interval", queue='indicators')
def maintain_symbol_interval(
    self: Task, symbol: str, interval: str
) -> dict:
    """
    ê°œë³„ ì‹¬ë³¼/ì¸í„°ë²Œì— ëŒ€í•œ ìœ ì§€ë³´ìˆ˜ íƒœìŠ¤í¬ (ë³‘ë ¬ ì‹¤í–‰ìš©)
    """
    if not is_pipeline_active():
        return {"status": "SKIP_PIPELINE_OFF"}

    # run_idëŠ” maintain_symbol_interval ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ì§€ ì•Šê³ ,
    # run_indicator_maintenanceì—ì„œ ìƒì„±í•˜ì—¬ ì¸ìë¡œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì´ ë” ì í•©í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” run_idë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.
    run_id = f"ind-{uuid.uuid4().hex}" # ì„ì‹œ run_id ìƒì„± (ì‹¤ì œë¡œëŠ” run_indicator_maintenanceì—ì„œ ë„˜ê²¨ë°›ì•„ì•¼ í•¨)

    try:
        # ğŸš€ Full Load Strategy (VRAM ì œì•½ ë¬´ì‹œ)
        # ëª¨ë“  ì¸í„°ë²Œì— ëŒ€í•´ ì¼ê´„ ì²˜ë¦¬
        
        # ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡
        upsert_indicator_progress(
            run_id, symbol, interval, "PROGRESS", 0.0, None, None
        )
        
        # ì „ì²´ ë°ì´í„° ë¡œë“œ ë° ê³„ì‚°
        saved_count = _process_indicator_full(
            symbol, interval, run_id=run_id
        )
        
        if not is_pipeline_active():
            return {"status": "ABORTED"}
        
        last_ts = _get_last_indicator_timestamp(symbol, interval)
        upsert_indicator_progress(
            run_id, symbol, interval, "SUCCESS", 100.0, last_ts, None
        )

        return {"status": "COMPLETE", "symbol": symbol, "interval": interval}

    except Exception as e:
        msg = f"maintain failed for {symbol} {interval}: {e}"
        logger.error(f"[Indicator] {msg}")
        upsert_indicator_progress(
            run_id, symbol, interval, "FAILURE", 0.0, None, msg
        )
        # ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡
        try:
            set_component_error(PipelineComponent.INDICATOR, msg)
        except:
            pass
        raise

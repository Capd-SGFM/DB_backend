import time
from datetime import datetime, timezone
from typing import Tuple, Optional

import httpx
import pandas as pd
from celery import Task
from loguru import logger
from sqlalchemy import select, func, text
from sqlalchemy.dialects.postgresql import insert

from db_module.connect_sqlalchemy_engine import SyncSessionLocal
from models import OHLCV_MODELS  # ğŸ”¹ ë³´ì¡°ì§€í‘œ ëª¨ë¸ì€ ë°±í•„ì—ì„œ ì‚¬ìš© ì•ˆ í•¨
from models.backfill_progress import BackfillProgress
from models.pipeline_state import (
    is_pipeline_active,
    set_component_error,
    PipelineComponent,
)
from . import celery_app

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Binance API ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BINANCE_FAPI_URL = "https://fapi.binance.com/fapi/v1/klines"
KLINE_LIMIT = 1000

# Rate Limit ì œì–´ìš© ìƒìˆ˜ (ë°”ì´ë‚¸ìŠ¤ ê³µì‹ ê¸°ì¤€)
RATE_LIMIT_HEADER = "x-mbx-used-weight-1m"
MAX_WEIGHT_PER_MINUTE = 2400
SAFETY_MARGIN_PERCENT = 0.8
TARGET_WEIGHT = MAX_WEIGHT_PER_MINUTE * SAFETY_MARGIN_PERCENT

# ì¸í„°ë²Œë³„ ë°€ë¦¬ì´ˆ
INTERVAL_TO_MS = {
    "1m": 60_000,
    "3m": 180_000,
    "5m": 300_000,
    "15m": 900_000,
    "30m": 1_800_000,
    "1h": 3_600_000,
    "4h": 14_400_000,
    "1d": 86_400_000,
    "1w": 7 * 86_400_000,
    "1M": 30 * 86_400_000,  # Binance 1M ì •ì˜ëŠ” ì‹¤ì œ ë‹¬ì´ì§€ë§Œ, interval ê¸¸ì´ ì¶”ì •ìš©
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  í—¬í¼: backfill_progress upsert
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def upsert_backfill_progress(
    run_id: str,
    symbol: str,
    interval: str,
    state: str,
    pct_time: float,
    last_candle_ts: Optional[datetime],
    last_error: Optional[str],
):
    """
    trading_data.backfill_progress ì— í˜„ì¬ ìƒíƒœë¥¼ UPSERT.
    """
    if not run_id:
        return

    with SyncSessionLocal() as session, session.begin():
        stmt = insert(BackfillProgress).values(
            run_id=run_id,
            symbol=symbol,
            interval=interval,
            state=state,
            pct_time=pct_time,
            last_candle_ts=last_candle_ts,
            last_error=last_error,
        )
        stmt = stmt.on_conflict_do_update(
            index_elements=["run_id", "symbol", "interval"],
            set_={
                "state": stmt.excluded.state,
                "pct_time": stmt.excluded.pct_time,
                "last_candle_ts": stmt.excluded.last_candle_ts,
                "last_error": stmt.excluded.last_error,
                "updated_at": text("now()"),
            },
        )
        session.execute(stmt)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  DBì—ì„œ REST ë°±í•„ ì‹œì‘ ì‹œê° ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_start_time_ms(
    symbol: str,
    interval: str,
    OhlcvModel,
    ws_frontier_ms: int,
) -> Tuple[Optional[int], bool]:
    """
    ws_frontier_ms(ë°€ë¦¬ì´ˆ) ì´ì „ êµ¬ê°„ì—ì„œ,
    is_ended = TRUE ì¸ ë§ˆì§€ë§‰ ìº”ë“¤ì˜ ë‹¤ìŒ ìº”ë“¤ë¶€í„° ë°±í•„í•˜ê¸° ìœ„í•´
    startTime(ms)ë¥¼ ê³„ì‚°í•œë‹¤.

    return:
        (start_time_ms, has_any_row)
        - start_time_ms: None ì´ë©´ 'is_ended=TRUE ê¸°ì¤€ ì‹œì‘ì ì„ ì°¾ì§€ ëª»í–ˆë‹¤'ëŠ” ì˜ë¯¸
        - has_any_row: ì´ ì‹¬ë³¼/ì¸í„°ë²Œì— rowê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ ì—¬ë¶€
    """
    ws_frontier_dt = datetime.fromtimestamp(ws_frontier_ms / 1000, tz=timezone.utc)

    with SyncSessionLocal() as session:
        # ì´ ì‹¬ë³¼/ì¸í„°ë²Œì— rowê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ (ë°ì´í„°ëŠ” ìˆëŠ”ë° endedê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ)
        total_count = session.execute(
            select(func.count()).where(OhlcvModel.symbol == symbol)
        ).scalar_one()
        has_any_row = total_count > 0

        # ws_frontier ì´ì „ì—ì„œ is_ended = TRUEì¸ ë§ˆì§€ë§‰ ìº”ë“¤
        stmt = select(func.max(OhlcvModel.timestamp)).where(
            OhlcvModel.symbol == symbol,
            OhlcvModel.is_ended == True,  # noqa: E712
            OhlcvModel.timestamp < ws_frontier_dt,
        )
        latest_timestamp: Optional[datetime] = session.execute(
            stmt
        ).scalar_one_or_none()

        if latest_timestamp:
            interval_ms = INTERVAL_TO_MS.get(interval, 60_000)
            start_ms = int(latest_timestamp.timestamp() * 1000) + interval_ms
            return start_ms, has_any_row
        else:
            # ended ìº”ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ì‹œì‘ì ì€ ì—†ìŒ
            return None, has_any_row


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  OHLCV ì €ì¥ (REST: í•­ìƒ is_ended = TRUE ê¸°ì¤€)
#   âš ï¸ ë°±í•„ì—ì„œëŠ” ë³´ì¡°ì§€í‘œë¥¼ ê³„ì‚°/ì €ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤!
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_data(OhlcvModel, symbol: str, all_klines: list) -> int:
    """
    all_klines:
      [
        {
          "symbol": str,
          "open_time_ms": int,
          "open": float,
          "high": float,
          "low": float,
          "close": float,
          "volume": float,
          "is_ended": bool,  # RESTì—ì„œëŠ” í•­ìƒ True
        }, ...
      ]

    ë°±í•„ ë‹¨ê³„ì—ì„œëŠ” OHLCV í…Œì´ë¸”ë§Œ UPSERTí•˜ê³ ,
    ë³´ì¡°ì§€í‘œ(indicators_*)ëŠ” ë³„ë„ì˜ ë³´ì¡°ì§€í‘œ ì—”ì§„ì—ì„œ ê³„ì‚°/ì €ì¥í•œë‹¤.
    """
    if not all_klines:
        return 0

    # DataFrame ë³€í™˜
    df = pd.DataFrame(all_klines)
    df["timestamp"] = pd.to_datetime(df["open_time_ms"], unit="ms", utc=True)

    # OHLCV ì €ì¥ìš©
    ohlcv_data_to_save = df[
        ["symbol", "timestamp", "open", "high", "low", "close", "volume", "is_ended"]
    ].to_dict("records")

    # ---- DB ì €ì¥ (OHLCV UPSERTë§Œ ìˆ˜í–‰) ----
    with SyncSessionLocal() as session:
        with session.begin():
            if ohlcv_data_to_save:
                ohlcv_stmt = insert(OhlcvModel).values(ohlcv_data_to_save)
                ohlcv_keys = ohlcv_data_to_save[0].keys()
                update_ohlcv_cols = {
                    key: getattr(ohlcv_stmt.excluded, key)
                    for key in ohlcv_keys
                    if key not in ["symbol", "timestamp"]
                }
                ohlcv_stmt = ohlcv_stmt.on_conflict_do_update(
                    index_elements=["symbol", "timestamp"],
                    set_=update_ohlcv_cols,
                )
                session.execute(ohlcv_stmt)

    return len(ohlcv_data_to_save)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  REST ë°±í•„ Celery Task
#   - ws_frontier_ms ì´ì „ êµ¬ê°„ë§Œ ìˆ˜ì§‘
#   - RESTë¡œ ì €ì¥í•˜ëŠ” ìº”ë“¤ì€ í•­ìƒ is_ended = TRUE
#   - íŒŒì´í”„ë¼ì¸(id=1)ì´ OFF ë˜ë©´ ì¤‘ê°„ì—ë„ ì¢…ë£Œ
#   - ì˜ˆì™¸ ë°œìƒ ì‹œ pipeline_state(BACKFILL)ì˜ last_errorì— ê¸°ë¡
#   - âš ï¸ ë°±í•„ì—ì„œëŠ” indicators_* í…Œì´ë¸”ì— ì „í˜€ ì“°ì§€ ì•ŠìŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@celery_app.task(bind=True, name="ohlcv.backfill_symbol_interval")
def backfill_symbol_interval(
    self: Task,
    symbol: str,
    pair: str,
    interval: str,
    ws_frontier_ms: Optional[int] = None,  # âœ… Optional ë¡œ ë³€ê²½
    run_id: str | None = None,
):
    """
    REST ë°±í•„ íƒœìŠ¤í¬ (ë™ê¸°):
      - ws_frontier_ms ì´ì „ êµ¬ê°„ë§Œ ë°±í•„
      - DB ìƒíƒœì— ë”°ë¼:
        1) í•´ë‹¹ ì‹¬ë³¼/ì¸í„°ë²Œì— is_ended = TRUE ìº”ë“¤ì´ ìˆìœ¼ë©´ â†’ ê·¸ ì´í›„ë¶€í„° ws_frontier ì§ì „ê¹Œì§€ ì¦ë¶„ ë°±í•„
        2) rowëŠ” ìˆëŠ”ë° is_ended = TRUEê°€ í•˜ë‚˜ë„ ì—†ê±°ë‚˜, ì•„ì˜ˆ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ â†’
           Binanceì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ê°€ì¥ ì˜¤ë˜ëœ ìº”ë“¤ë¶€í„° ws_frontier ì§ì „ê¹Œì§€ ì „ì²´ ë°±í•„
      - RESTê°€ ì €ì¥í•˜ëŠ” ëª¨ë“  ìº”ë“¤ì€ is_ended = TRUE ë¡œ ì €ì¥
      - ë³´ì¡°ì§€í‘œ(indicators_*) ì €ì¥ì€ í•˜ì§€ ì•ŠìŒ (ë³„ë„ ì—”ì§„ ë‹´ë‹¹)
    """
    OhlcvModel = OHLCV_MODELS.get(interval)
    if not OhlcvModel:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸í„°ë²Œì…ë‹ˆë‹¤: {interval}")

    # í•œë²ˆì— ë©”ëª¨ë¦¬ì— ë³´ê´€í–ˆë‹¤ ì €ì¥í•  ìµœëŒ€ ê°œìˆ˜ (ëŒ€ëŸ‰ ìˆ˜ì§‘ ëŒ€ë¹„)
    BATCH_SAVE_SIZE = 50_000
    all_klines_data = []
    total_saved_count = 0
    last_known_pct = 0.0

    try:
        # â”€â”€â”€â”€â”€ íŒŒì´í”„ë¼ì¸ OFF ë©´ ë°”ë¡œ ì¢…ë£Œ â”€â”€â”€â”€â”€
        if not is_pipeline_active():
            logger.info(f"[{symbol}-{interval}] pipeline inactive â†’ skip backfill.")
            if run_id:
                upsert_backfill_progress(
                    run_id,
                    symbol,
                    interval,
                    state="PENDING",
                    pct_time=0.0,
                    last_candle_ts=None,
                    last_error=None,
                )
            return {
                "status": "SKIP",
                "symbol": symbol,
                "interval": interval,
                "saved_count": 0,
            }

        # â”€â”€â”€â”€â”€ 0. ws_frontier_ms ì—†ìœ¼ë©´ Binance serverTime ìœ¼ë¡œ ìë™ ì„¤ì • â”€â”€â”€â”€â”€
        if ws_frontier_ms is None:
            with httpx.Client(timeout=30.0) as client:
                time_res = client.get("https://fapi.binance.com/fapi/v1/time")
                time_res.raise_for_status()
                ws_frontier_ms = int(time_res.json()["serverTime"])

        # â”€â”€â”€â”€â”€ 1. DB ìƒíƒœ + ws_frontier ê¸°ë°˜ ì‹œì‘ ì‹œê° ê²°ì • â”€â”€â”€â”€â”€
        db_start_time_ms, has_any_row = get_start_time_ms(
            symbol, interval, OhlcvModel, ws_frontier_ms
        )

        current_start_time_ms: Optional[int] = None
        progress_start_ms: Optional[int] = None
        progress_end_ms: int = ws_frontier_ms  # REST ë°±í•„ ëª©í‘œ êµ¬ê°„ ëì 

        if has_any_row and db_start_time_ms is not None:
            # (1) ë°ì´í„° ìˆê³  is_ended = TRUEë„ ìˆìŒ â†’ ë§ˆì§€ë§‰ ended ìº”ë“¤ ì´í›„ë¶€í„° ì‹œì‘
            logger.info(
                f"[{symbol}-{interval}] ì¦ë¶„ ë°±í•„ ì‹œì‘ (db_start_time_ms={db_start_time_ms}, ws_frontier_ms={ws_frontier_ms})"
            )
            current_start_time_ms = db_start_time_ms
            progress_start_ms = db_start_time_ms

        else:
            # (2) rowëŠ” ìˆëŠ”ë° is_ended=TRUEê°€ í•˜ë‚˜ë„ ì—†ê±°ë‚˜, ì•„ì˜ˆ ë°ì´í„°ê°€ ì—†ìŒ â†’ ì „ì²´ ë°±í•„
            logger.info(
                f"[{symbol}-{interval}] ì „ì²´ ë°±í•„ ì‹œì‘ (DB ë¹„ì–´ìˆê±°ë‚˜ is_ended=TRUE ì—†ìŒ). "
                "Binanceì—ì„œ ì‹¤ì œ ì²« ìº”ë“¤ ì‹œê°„ ì¡°íšŒ..."
            )
            with httpx.Client(timeout=30.0) as client:
                params = {
                    "symbol": pair,
                    "interval": interval,
                    "startTime": 1,
                    "limit": 1,
                }
                res = client.get(BINANCE_FAPI_URL, params=params)
                res.raise_for_status()
                first_candle_data = res.json()

                if not first_candle_data:
                    logger.warning(
                        f"[{symbol}-{interval}] APIì— ë°ì´í„°ê°€ ì „í˜€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì¢…ë£Œ."
                    )
                    if run_id:
                        upsert_backfill_progress(
                            run_id,
                            symbol,
                            interval,
                            state="SUCCESS",
                            pct_time=100.0,
                            last_candle_ts=None,
                            last_error=None,
                        )
                    return {
                        "status": "COMPLETE",
                        "symbol": symbol,
                        "interval": interval,
                        "saved_count": 0,
                    }

                actual_first_candle_ms = int(first_candle_data[0][0])
                current_start_time_ms = actual_first_candle_ms
                progress_start_ms = actual_first_candle_ms

                first_candle_dt = datetime.fromtimestamp(
                    actual_first_candle_ms / 1000, tz=timezone.utc
                )
                logger.info(
                    f"[{symbol}-{interval}] ì‹¤ì œ ì‹œì‘ ì‹œê°„ í™•ì¸: {first_candle_dt.isoformat()}, "
                    f"ws_frontier_ms={ws_frontier_ms}"
                )

        if current_start_time_ms is None or progress_start_ms is None:
            raise Exception("Start time could not be determined.")

        # ì´ë¯¸ ws_frontierë³´ë‹¤ ë’¤ë©´ í•  ì¼ì´ ì—†ìŒ
        if current_start_time_ms >= ws_frontier_ms:
            logger.info(
                f"[{symbol}-{interval}] current_start_time_ms({current_start_time_ms}) >= ws_frontier_ms({ws_frontier_ms}) "
                "â†’ ë°±í•„í•  êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤."
            )
            if run_id:
                upsert_backfill_progress(
                    run_id,
                    symbol,
                    interval,
                    state="SUCCESS",
                    pct_time=100.0,
                    last_candle_ts=None,
                    last_error=None,
                )
            return {
                "status": "COMPLETE",
                "symbol": symbol,
                "interval": interval,
                "saved_count": 0,
            }

        interval_ms = INTERVAL_TO_MS.get(interval, 60_000)

        # â”€â”€â”€â”€â”€ 2. Binance REST ë£¨í”„ (ws_frontier ì´ì „ê¹Œì§€ë§Œ) â”€â”€â”€â”€â”€
        with httpx.Client(timeout=30.0) as client:
            while True:
                # íŒŒì´í”„ë¼ì¸ì´ ì¤‘ê°„ì— êº¼ì§€ë©´ ì¢…ë£Œ
                if not is_pipeline_active():
                    logger.info(
                        f"[{symbol}-{interval}] pipeline OFF ê°ì§€ â†’ backfill ì¤‘ë‹¨."
                    )
                    if run_id:
                        upsert_backfill_progress(
                            run_id,
                            symbol,
                            interval,
                            state="PENDING",
                            pct_time=last_known_pct,
                            last_candle_ts=None,
                            last_error=None,
                        )
                    break

                # ë” ì´ìƒ ìˆ˜ì§‘í•  êµ¬ê°„ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                if current_start_time_ms >= ws_frontier_ms:
                    logger.info(
                        f"[{symbol}-{interval}] ws_frontier({ws_frontier_ms})ê¹Œì§€ ëª¨ë‘ ìˆ˜ì§‘í•˜ì—¬ ì¢…ë£Œ."
                    )
                    break

                params = {
                    "symbol": pair,
                    "interval": interval,
                    "limit": KLINE_LIMIT,
                    "startTime": current_start_time_ms,
                    "endTime": ws_frontier_ms - 1,  # WebSocket ë‹´ë‹¹ êµ¬ê°„ ì§ì „ê¹Œì§€ë§Œ
                }

                try:
                    res = client.get(BINANCE_FAPI_URL, params=params)

                    # ë ˆì´íŠ¸ ë¦¬ë°‹ ë³´í˜¸
                    if res.status_code in (429, 418):
                        retry_after = res.headers.get("Retry-After")
                        sleep_time = 60
                        if retry_after and retry_after.isdigit():
                            sleep_time = int(retry_after)

                        logger.warning(
                            f"[{symbol}-{interval}] Rate limit hit (Status {res.status_code}). "
                            f"Sleeping for {sleep_time} seconds..."
                        )
                        self.update_state(
                            state="PROGRESS",
                            meta={
                                "symbol": symbol,
                                "interval": interval,
                                "pct": last_known_pct,
                                "last_candle_time": datetime.fromtimestamp(
                                    current_start_time_ms / 1000, tz=timezone.utc
                                ).isoformat(),
                                "status": f"Rate limit. Paused for {sleep_time}s.",
                            },
                        )
                        if run_id:
                            upsert_backfill_progress(
                                run_id,
                                symbol,
                                interval,
                                state="PROGRESS",
                                pct_time=last_known_pct,
                                last_candle_ts=datetime.fromtimestamp(
                                    current_start_time_ms / 1000, tz=timezone.utc
                                ),
                                last_error=None,
                            )
                        time.sleep(sleep_time)
                        continue

                    res.raise_for_status()
                    klines = res.json()

                except httpx.HTTPStatusError as e:
                    logger.error(f"[{symbol}-{interval}] HTTP Error: {e}")
                    raise Exception(f"HTTP Error: {e.response.status_code}")
                except httpx.RequestError as e:
                    logger.error(f"[{symbol}-{interval}] Connection Error: {e}")
                    raise Exception(f"Connection Error: {e}")

                if not klines:
                    logger.info(
                        f"[{symbol}-{interval}] APIê°€ ë¹ˆ ëª©ë¡ì„ ë°˜í™˜. ë£¨í”„ ì¢…ë£Œ."
                    )
                    break

                new_klines_count = 0
                last_saved_open_ms: Optional[int] = None

                for k in klines:
                    open_time_ms = int(k[0])

                    # ë°©ì–´ì : ws_frontier ì´í›„ì˜ ìº”ë“¤ì€ REST ë°±í•„ ëŒ€ìƒì´ ì•„ë‹˜
                    if open_time_ms >= ws_frontier_ms:
                        continue

                    # ì¦ë¶„ ëª¨ë“œë¼ë©´, ì´ì „ ended ì´í›„ ì‹œì ë§Œ
                    if db_start_time_ms and open_time_ms < db_start_time_ms:
                        continue

                    all_klines_data.append(
                        {
                            "symbol": symbol,
                            "open_time_ms": open_time_ms,
                            "open": float(k[1]),
                            "high": float(k[2]),
                            "low": float(k[3]),
                            "close": float(k[4]),
                            "volume": float(k[5]),
                            # RESTë¡œ ë“¤ì–´ì˜¤ëŠ” ìº”ë“¤ì€ ëª¨ë‘ 'ë‹«íŒ ìº”ë“¤'ë¡œ ë³´ê³  is_ended = TRUE
                            "is_ended": True,
                        }
                    )
                    new_klines_count += 1
                    last_saved_open_ms = open_time_ms

                # ì§„í–‰ë¥  ê³„ì‚° (ws_frontier ê¸°ì¤€)
                if (
                    last_saved_open_ms is not None
                    and progress_start_ms is not None
                    and progress_end_ms > progress_start_ms
                ):
                    pct = (
                        (last_saved_open_ms - progress_start_ms)
                        / (progress_end_ms - progress_start_ms)
                    ) * 100
                else:
                    pct = 0

                last_known_pct = min(round(pct, 2), 100.0)

                last_ts = datetime.fromtimestamp(
                    (last_saved_open_ms or current_start_time_ms) / 1000,
                    tz=timezone.utc,
                )

                self.update_state(
                    state="PROGRESS",
                    meta={
                        "symbol": symbol,
                        "interval": interval,
                        "pct": last_known_pct,
                        "last_candle_time": last_ts.isoformat(),
                        "status": "Running...",
                    },
                )

                if run_id:
                    upsert_backfill_progress(
                        run_id,
                        symbol,
                        interval,
                        state="PROGRESS",
                        pct_time=last_known_pct,
                        last_candle_ts=last_ts,
                        last_error=None,
                    )

                # ë©”ëª¨ë¦¬ ë°°ì¹˜ ì €ì¥
                if len(all_klines_data) >= BATCH_SAVE_SIZE:
                    logger.info(
                        f"[{symbol}-{interval}] ë©”ëª¨ë¦¬ ë°°ì¹˜ {len(all_klines_data)}ê°œ ì €ì¥ ì‹œë„..."
                    )
                    saved_in_batch = save_data(OhlcvModel, symbol, all_klines_data)
                    total_saved_count += saved_in_batch
                    all_klines_data.clear()

                # ìƒˆë¡œ ì €ì¥ëœ ìº”ë“¤ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´ ì¢…ë£Œ
                if new_klines_count == 0:
                    logger.info(
                        f"[{symbol}-{interval}] ìƒˆë¡œ ì €ì¥ëœ ìº”ë“¤ì´ ì—†ìœ¼ë¯€ë¡œ ë£¨í”„ ì¢…ë£Œ."
                    )
                    break

                # ë‹¤ìŒ ë°°ì¹˜ ì‹œì‘ ì‹œê° ê²°ì •
                if last_saved_open_ms is None:
                    logger.info(
                        f"[{symbol}-{interval}] last_saved_open_msê°€ ì—†ì–´ ë£¨í”„ ì¢…ë£Œ."
                    )
                    break

                current_start_time_ms = last_saved_open_ms + interval_ms

                # ws_frontierì— ë„ë‹¬í–ˆìœ¼ë©´ ì¢…ë£Œ
                if current_start_time_ms >= ws_frontier_ms:
                    logger.info(
                        f"[{symbol}-{interval}] ws_frontier({ws_frontier_ms})ê¹Œì§€ ìˆ˜ì§‘ ì™„ë£Œ."
                    )
                    break

                # Rate Limit ì™„í™”ìš© weight ì²´í¬
                try:
                    used_weight = int(res.headers.get(RATE_LIMIT_HEADER, "0"))
                    if used_weight > TARGET_WEIGHT:
                        sleep_duration = 10
                        logger.warning(
                            f"[{symbol}-{interval}] High weight ({used_weight}). "
                            f"Pausing for {sleep_duration}s."
                        )
                        self.update_state(
                            state="PROGRESS",
                            meta={
                                "symbol": symbol,
                                "interval": interval,
                                "pct": last_known_pct,
                                "last_candle_time": datetime.fromtimestamp(
                                    last_saved_open_ms / 1000, tz=timezone.utc
                                ).isoformat(),
                                "status": f"Pacing weight. Paused for {sleep_duration}s.",
                            },
                        )
                        if run_id:
                            upsert_backfill_progress(
                                run_id,
                                symbol,
                                interval,
                                state="PROGRESS",
                                pct_time=last_known_pct,
                                last_candle_ts=datetime.fromtimestamp(
                                    last_saved_open_ms / 1000, tz=timezone.utc
                                ),
                                last_error=None,
                            )
                        time.sleep(sleep_duration)
                except Exception:
                    time.sleep(0.5)

        # â”€â”€â”€â”€â”€ 3. ë‚¨ì€ ë©”ëª¨ë¦¬ ë°°ì¹˜ ì €ì¥ â”€â”€â”€â”€â”€
        if all_klines_data:
            logger.info(
                f"[{symbol}-{interval}] ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ {len(all_klines_data)}ê°œ ìº”ë“¤ ì €ì¥ ì‹œë„..."
            )
            saved_in_batch = save_data(OhlcvModel, symbol, all_klines_data)
            total_saved_count += saved_in_batch
            all_klines_data.clear()

        # ì„±ê³µì ìœ¼ë¡œ ëë‚œ ê²½ìš° ìƒíƒœ SUCCESS ë¡œ ê¸°ë¡
        if run_id:
            upsert_backfill_progress(
                run_id,
                symbol,
                interval,
                state="SUCCESS",
                pct_time=100.0,
                last_candle_ts=None,
                last_error=None,
            )

        return {
            "status": "COMPLETE",
            "symbol": symbol,
            "interval": interval,
            "saved_count": total_saved_count,
        }

    except Exception as e:
        logger.error(
            f"Task {getattr(self.request, 'id', 'unknown')} "
            f"(Symbol: {symbol}, Interval: {interval}) failed: {e}"
        )
        # BACKFILL ì»´í¬ë„ŒíŠ¸ ì—ëŸ¬ ê¸°ë¡
        try:
            set_component_error(
                PipelineComponent.BACKFILL,
                f"{type(e).__name__}: {e}",
            )
        except Exception:
            logger.exception("[BACKFILL] failed to save last_error")

        # backfill_progress ì— FAILURE ê¸°ë¡
        if run_id:
            upsert_backfill_progress(
                run_id,
                symbol,
                interval,
                state="FAILURE",
                pct_time=last_known_pct,
                last_candle_ts=None,
                last_error=str(e),
            )

        raise Exception(f"Task failed for {symbol} {interval}: {str(e)}")

# celery_task/pipeline_task.py
from __future__ import annotations

import time
import uuid

import httpx
from celery import group, chain, chord
from loguru import logger
from sqlalchemy import select, func
from sqlalchemy.dialects.postgresql import insert

from . import celery_app
from . import websocket_task
from .rest_api_task import backfill_symbol_interval

from db_module.connect_sqlalchemy_engine import SyncSessionLocal
from models import CryptoInfo
from models.pipeline_state import (
    is_pipeline_active,
    set_component_active,
    archive_current_errors,
    PipelineComponent,
)
from models.backfill_progress import BackfillProgress
from celery_task.rest_maintenance_task import run_rest_maintenance
from models.backfill_progress import BackfillProgress
from celery_task.rest_maintenance_task import run_rest_maintenance
from celery_task.indicator_task import (
    run_indicator_maintenance, 
    purge_indicators_queue, 
    stop_all_indicator_tasks
)

__all__ = ["start_pipeline", "stop_pipeline", "run_maintenance_cycle"]


# ================================================================
# 1) ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (WebSocket â†’ Backfill â†’ Maintenance)
# ================================================================
@celery_app.task(name="pipeline.start_pipeline")
def start_pipeline():
    logger.info("[pipeline] íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    if not is_pipeline_active():
        logger.info("[pipeline] pipeline_state.id=1 ì´ OFF â†’ ì¢…ë£Œ")
        return

    # -----------------------------
    # 1) WebSocket ì—”ì§„ ì‹œì‘
    # -----------------------------
    set_component_active(PipelineComponent.WEBSOCKET, True)
    websocket_task.websocket_collector.delay()
    logger.info("[pipeline] WebSocket collector started")

    # -----------------------------
    # 2) Backfill ì¤€ë¹„ ë° ì‹¤í–‰ (Chain)
    # -----------------------------
    # wait_for_ws_data -> prepare_backfill -> execute_backfill_chord
    chain(
        wait_for_ws_data.s(),
        prepare_backfill_and_execute.s()
    ).apply_async()


@celery_app.task(name="pipeline.wait_for_ws_data", bind=True, max_retries=10)
def wait_for_ws_data(self):
    """
    WebSocket ë°ì´í„°ê°€ ëª¨ë“  ì‹¬ë³¼/ì¸í„°ë²Œì— ëŒ€í•´ ì—°ê²°ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆ)
    """
    if not is_pipeline_active():
        return False

    logger.info("[pipeline] ëª¨ë“  WebSocket ì—°ê²° ëŒ€ê¸° ì¤‘...")

    # 1. ê¸°ëŒ€ë˜ëŠ” ì—°ê²° ìˆ˜ ê³„ì‚°
    with SyncSessionLocal() as session:
        # í™œì„±í™”ëœ ì‹¬ë³¼ ìˆ˜
        symbol_count = (
            session.query(func.count(CryptoInfo.symbol))
            .filter(CryptoInfo.pair.isnot(None))
            .scalar()
        )
    
    # ì¸í„°ë²Œ ìˆ˜ (10ê°œ)
    interval_count = 10  # ["1m", "3m", "5m", "15m", "30m", "1h", "4h", "1d", "1w", "1M"]
    total_expected = symbol_count * interval_count
    
    logger.info(f"[pipeline] ëª©í‘œ ì—°ê²° ìˆ˜: {total_expected} (Symbols={symbol_count}, Intervals={interval_count})")

    # 2. Polling (ìµœëŒ€ 60ì´ˆ, 2ì´ˆ ê°„ê²©)
    max_wait_seconds = 60
    check_interval = 2
    elapsed = 0
    
    from models.websocket_progress import WebSocketProgress  # ëŠ¦ì€ importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€ ê°€ëŠ¥ì„± ëŒ€ë¹„

    while elapsed < max_wait_seconds:
        if not is_pipeline_active():
            return False

        with SyncSessionLocal() as session:
            # ê°€ì¥ ìµœê·¼ run_id ì°¾ê¸° (í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ WS)
            # (ì£¼ì˜: ì—¬ëŸ¬ run_idê°€ ì„ì—¬ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ê°€ì¥ ìµœê·¼ì— ì—…ë°ì´íŠ¸ëœ run_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨)
            latest_run_id = (
                session.query(WebSocketProgress.run_id)
                .order_by(WebSocketProgress.updated_at.desc())
                .limit(1)
                .scalar()
            )
            
            if latest_run_id:
                # í•´ë‹¹ run_idì—ì„œ CONNECTED ìƒíƒœì¸ ê°œìˆ˜ ì¡°íšŒ
                connected_count = (
                    session.query(func.count())
                    .filter(
                        WebSocketProgress.run_id == latest_run_id,
                        WebSocketProgress.state == "CONNECTED"
                    )
                    .scalar()
                )
            else:
                connected_count = 0

        if connected_count >= total_expected:
            logger.info(f"[pipeline] ëª¨ë“  WebSocket ì—°ê²° ì™„ë£Œ ({connected_count}/{total_expected}). Backfill ì§„ì….")
            return True
        
        logger.info(f"[pipeline] WebSocket ì—°ê²° ëŒ€ê¸° ì¤‘... ({connected_count}/{total_expected}) - {elapsed}s")
        time.sleep(check_interval)
        elapsed += check_interval

    # íƒ€ì„ì•„ì›ƒ ë°œìƒ
    logger.error(f"[pipeline] WebSocket ì—°ê²° íƒ€ì„ì•„ì›ƒ ({max_wait_seconds}s). Backfillì„ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    # ì—¬ê¸°ì„œ Falseë¥¼ ë¦¬í„´í•˜ê±°ë‚˜ ì˜ˆì™¸ë¥¼ ë˜ì ¸ì„œ Chainì„ ì¤‘ë‹¨ì‹œì¼œì•¼ í•¨.
    # Chainì—ì„œ ì•ì˜ íƒœìŠ¤í¬ê°€ ì‹¤íŒ¨(ì˜ˆì™¸)í•˜ë©´ ë’¤ì˜ íƒœìŠ¤í¬ëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ.
    # í•˜ì§€ë§Œ return Falseë¡œëŠ” Chainì´ ë©ˆì¶”ì§€ ì•Šê³  ë‹¤ìŒ íƒœìŠ¤í¬ë¡œ ë„˜ì–´ê° (ì¸ìë¡œ Falseê°€ ì „ë‹¬ë¨).
    # ë”°ë¼ì„œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ê²ƒì´ í™•ì‹¤í•¨.
    raise RuntimeError("WebSocket connection timeout")


@celery_app.task(name="pipeline.prepare_backfill_and_execute")
def prepare_backfill_and_execute(prev_result):
    logger.info(f"[pipeline] prepare_backfill_and_execute called with prev_result={prev_result}")
    if not is_pipeline_active():
        logger.info("[pipeline] Pipeline inactive, skipping backfill preparation")
        return

    # -----------------------------
    # Binance ì„œë²„ ì‹œê°„ ì¡°íšŒ
    # -----------------------------
    try:
        with httpx.Client(timeout=10.0) as client:
            res = client.get("https://fapi.binance.com/fapi/v1/time")
            res.raise_for_status()
            server_time_ms = int(res.json()["serverTime"])
    except Exception as e:
        logger.error(f"[pipeline] serverTime ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ì´ í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ raiseí•˜ì—¬ retry
        return

    ws_frontier_ms = server_time_ms
    logger.info(f"[pipeline] ws_frontier_ms={ws_frontier_ms}")

    # -----------------------------
    # 3) Backfill ì‹œì‘
    # -----------------------------
    set_component_active(PipelineComponent.BACKFILL, True)
    run_id = f"run-{uuid.uuid4().hex}"
    logger.info(f"[pipeline] Backfill run_id={run_id}")

    # -----------------------------
    # ì‹¬ë³¼ ê°€ì ¸ì˜¤ê¸°
    # -----------------------------
    with SyncSessionLocal() as session:
        symbols = (
            session.query(CryptoInfo.symbol, CryptoInfo.pair)
            .filter(CryptoInfo.pair.isnot(None))
            .all()
        )

    intervals = ["1m", "5m", "15m", "1h", "4h", "1d"]

    # -----------------------------
    # BackfillProgress Dummy row ìƒì„±
    # -----------------------------
    with SyncSessionLocal() as session, session.begin():
        for sym, _pair in symbols:
            for interval in intervals:
                stmt = (
                    insert(BackfillProgress)
                    .values(
                        run_id=run_id,
                        symbol=sym,
                        interval=interval,
                        state="PENDING",
                        pct_time=0.0,
                        last_candle_ts=None,
                        last_error=None,
                    )
                    .on_conflict_do_nothing()
                )
                session.execute(stmt)

    logger.info("[pipeline] BackfillProgress dummy rows inserted")

    # -----------------------------
    # Backfill ë³‘ë ¬ ì¡ ìƒì„± (Chord)
    # -----------------------------
    header = []
    for sym, pair in symbols:
        for interval in intervals:
            header.append(
                backfill_symbol_interval.s(
                    symbol=sym,
                    pair=pair,
                    interval=interval,
                    ws_frontier_ms=ws_frontier_ms,
                    run_id=run_id,
                )
            )

    # Chord: ëª¨ë“  backfill ì‘ì—…ì´ ëë‚˜ë©´ on_backfill_complete ì‹¤í–‰
    callback = on_backfill_complete.s(run_id=run_id)
    chord(header)(callback)
    logger.info("[pipeline] Backfill chord started")


@celery_app.task(name="pipeline.on_backfill_complete")
def on_backfill_complete(results, run_id):
    """
    Backfill ì™„ë£Œ í›„ í˜¸ì¶œë˜ëŠ” ì½œë°±
    """
    logger.info(f"[pipeline] Backfill ì™„ë£Œ (run_id={run_id})")
    set_component_active(PipelineComponent.BACKFILL, False)

    if not is_pipeline_active():
        return

    # ì‹¤íŒ¨ í™•ì¸
    # resultsëŠ” ê° íƒœìŠ¤í¬ì˜ ë¦¬í„´ê°’ ë¦¬ìŠ¤íŠ¸
    # ì˜ˆ: [{'status': 'COMPLETE', ...}, {'status': 'SKIP'}, ...]
    
    # ê°„ë‹¨íˆ DBì—ì„œ ìµœì¢… ìƒíƒœ í™•ì¸
    if not is_backfill_success(run_id):
        logger.error("[pipeline] Backfill ì‹¤íŒ¨ í¬í•¨ë¨ â†’ Maintenance ì§„ì… ì¤‘ë‹¨")
        return

    logger.info("[pipeline] Backfill SUCCESS â†’ Maintenance ì‚¬ì´í´ ì‹œì‘")
    run_maintenance_cycle.delay()


def is_backfill_success(run_id: str) -> bool:
    with SyncSessionLocal() as session:
        rows = (
            session.execute(
                select(BackfillProgress.state).where(BackfillProgress.run_id == run_id)
            )
            .scalars()
            .all()
        )
    if not rows:
        return False
    if any(state in ("FAILURE", "FAIL") for state in rows):
        return False
    return True


# ================================================================
# 2) íŒŒì´í”„ë¼ì¸ OFF
# ================================================================
@celery_app.task(name="pipeline.stop_pipeline")
def stop_pipeline():
    logger.info("[pipeline] ì „ì²´ pipeline OFF")

    # ì—ëŸ¬ ë¡œê·¸ ì•„ì¹´ì´ë¹™
    try:
        archive_current_errors()
        logger.info("[pipeline] Current error logs archived to history")
    except Exception as e:
        logger.error(f"[pipeline] Failed to archive error logs: {e}")

    # ğŸš€ Emergency Stop Logic
    # 1. Purge 'indicators' queue
    purge_indicators_queue()
    
    # 2. Terminate running indicator tasks
    stop_all_indicator_tasks()

    # ê° ì»´í¬ë„ŒíŠ¸(Websocket, Maintenance)ëŠ” is_pipeline_active()ë¥¼ ì²´í¬í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ì¢…ë£Œë¨
    return


# ================================================================
# 3) Maintenance Cycle (Loop ëŒ€ì‹  ì¬ê·€ì  í˜¸ì¶œ or Periodic)
# ================================================================
@celery_app.task(name="pipeline.run_maintenance_cycle")
def run_maintenance_cycle():
    """
    REST ìœ ì§€ë³´ìˆ˜ -> Indicator ìœ ì§€ë³´ìˆ˜ -> (ì ì‹œ ëŒ€ê¸°) -> ë‹¤ì‹œ REST ìœ ì§€ë³´ìˆ˜ ...
    ë¬´í•œ ë£¨í”„ ëŒ€ì‹  Chain + ì¬ê·€ í˜¸ì¶œë¡œ êµ¬í˜„í•˜ì—¬ Worker Blocking ë°©ì§€
    """
    if not is_pipeline_active():
        logger.info("[pipeline] Maintenance cycle stopped (Pipeline inactive)")
        return

    logger.info("[pipeline] Maintenance cycle step started")
    
    # Chain: REST -> Indicator -> Next Cycle
    chain(
        run_rest_maintenance_wrapper.s(),
        run_indicator_maintenance_wrapper.s(),
        schedule_next_maintenance.s()
    ).apply_async()


@celery_app.task(name="pipeline.run_rest_maintenance_wrapper")
def run_rest_maintenance_wrapper():
    if not is_pipeline_active():
        return {"status": "INACTIVE"}
    
    set_component_active(PipelineComponent.REST_MAINTENANCE, True)
    logger.info("[pipeline] REST ìœ ì§€ë³´ìˆ˜ ì‹œì‘")
    
    # ë™ê¸° í•¨ìˆ˜ì¸ run_rest_maintenanceë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜, 
    # ë§Œì•½ run_rest_maintenanceê°€ async taskë¼ë©´ .delay()ê°€ ì•„ë‹ˆë¼ ì§ì ‘ í˜¸ì¶œí•´ì•¼ í•¨.
    # í˜„ì¬ rest_maintenance_task.pyì˜ run_rest_maintenanceëŠ” @shared_taskì„.
    # ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜, 
    # ì•„ë‹ˆë©´ chainì„ ë” ìª¼ê°œì•¼ í•¨.
    # run_rest_maintenanceê°€ ë‚´ë¶€ì ìœ¼ë¡œ blockingì´ë©´ ì—¬ê¸°ì„œë„ blockingë¨.
    # í•˜ì§€ë§Œ run_rest_maintenanceëŠ” ë‚´ë¶€ì—ì„œ loopë¥¼ ëŒë©° ì²˜ë¦¬í•˜ë¯€ë¡œ 
    # í•˜ë‚˜ì˜ ê±°ëŒ€í•œ taskì„. 
    # ì´ë¥¼ ìª¼ê°œëŠ” ê±´ ë” í° ê³µì‚¬ë¯€ë¡œ, ì¼ë‹¨ì€ wrapperì—ì„œ í˜¸ì¶œí•˜ê³  ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼.
    
    # ì£¼ì˜: run_rest_maintenanceê°€ celery taskë¡œ ì •ì˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,
    # í•¨ìˆ˜ë¡œ ì§ì ‘ í˜¸ì¶œí•˜ë©´ ì¼ë°˜ í•¨ìˆ˜ì²˜ëŸ¼ ì‹¤í–‰ë¨ (Celery app context ë‚´ì—ì„œ).
    # ë‹¨, @shared_task ë°ì½”ë ˆì´í„°ê°€ ìˆìœ¼ë©´ .delay() ì—†ì´ í˜¸ì¶œ ì‹œ ì›ë³¸ í•¨ìˆ˜ ì‹¤í–‰ë¨.
    
    try:
        result = run_rest_maintenance() # ì§ì ‘ í˜¸ì¶œ (ë™ê¸° ì‹¤í–‰)
    except Exception as e:
        logger.error(f"[pipeline] REST maintenance error: {e}")
        result = {"status": "FAILURE"}
        
    set_component_active(PipelineComponent.REST_MAINTENANCE, False)
    logger.info("[pipeline] REST ìœ ì§€ë³´ìˆ˜ ì¢…ë£Œ")
    return result


@celery_app.task(name="pipeline.run_indicator_maintenance_wrapper")
def run_indicator_maintenance_wrapper(prev_result):
    if not is_pipeline_active():
        return {"status": "INACTIVE"}

    set_component_active(PipelineComponent.INDICATOR, True)
    logger.info("[pipeline] Indicator ê³„ì‚° ì‹œì‘")
    
    try:
        tasks = run_indicator_maintenance() # íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        if not tasks:
            logger.info("[pipeline] ì‹¤í–‰í•  Indicator íƒœìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            set_component_active(PipelineComponent.INDICATOR, False)
            return {"status": "NO_TASKS"}
            
        # Chord ì‹¤í–‰: tasks -> on_indicator_complete
        # ì£¼ì˜: run_indicator_maintenance_wrapperëŠ” ì—¬ê¸°ì„œ ì¢…ë£Œë˜ì§€ë§Œ, 
        # ìƒíƒœ(Active)ëŠ” on_indicator_completeì—ì„œ êº¼ì•¼ í•¨.
        
        callback = on_indicator_complete.s()
        chord(tasks)(callback)
        
        logger.info(f"[pipeline] Indicator chord started ({len(tasks)} tasks)")
        return {"status": "STARTED", "task_count": len(tasks)}

    except Exception as e:
        logger.error(f"[pipeline] Indicator maintenance error: {e}")
        set_component_active(PipelineComponent.INDICATOR, False)
        return {"status": "FAILURE"}


@celery_app.task(name="pipeline.on_indicator_complete")
def on_indicator_complete(results):
    """
    Indicator ê³„ì‚° ì™„ë£Œ í›„ í˜¸ì¶œ
    """
    logger.info("[pipeline] Indicator ê³„ì‚° ì™„ë£Œ (All tasks finished)")
    set_component_active(PipelineComponent.INDICATOR, False)


@celery_app.task(name="pipeline.schedule_next_maintenance")
def schedule_next_maintenance(prev_result):
    if not is_pipeline_active():
        return
    
    # 1ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ ì‚¬ì´í´ ìŠ¤ì¼€ì¤„ë§
    # ì¬ê·€ í˜¸ì¶œ (countdown ì‚¬ìš©)
    run_maintenance_cycle.apply_async(countdown=1)


# ================================================================
# 4) Backtesting Pipeline (Separate)
# ================================================================
@celery_app.task(name="pipeline.start_backtesting")
def start_backtesting_pipeline():
    from models.pipeline_state import _get_state
    from celery import chord
    from .rest_api_task import backfill_backtesting_symbol_interval

    logger.info("[pipeline] start_backtesting_pipeline called")

    # Check state
    st = _get_state(PipelineComponent.BACKTESTING)
    if not st or not st.is_active:
        logger.info("[pipeline] Backtesting pipeline inactive, aborting.")
        return

    # Generate Run ID
    run_id = f"backtest-{uuid.uuid4().hex}"
    logger.info(f"[pipeline] Backtesting run_id={run_id}")

    # Fetch Symbols (is_backtesting_only=True)
    with SyncSessionLocal() as session:
        symbols = (
            session.query(CryptoInfo.symbol, CryptoInfo.pair)
            .filter(CryptoInfo.pair.isnot(None), CryptoInfo.is_backtesting_only.is_(True))
            .all()
        )
    
    if not symbols:
         logger.info("[pipeline] No backtesting-only symbols found.")
         # Should we turn off the pipeline? Maybe not, just stay active.
         return

    intervals = ["1m", "5m", "15m", "1h", "4h", "1d"]
    
    # Init BackfillProgress
    with SyncSessionLocal() as session, session.begin():
        for sym, _pair in symbols:
            for interval in intervals:
                stmt = (
                    insert(BackfillProgress)
                    .values(
                        run_id=run_id,
                        symbol=sym,
                        interval=interval,
                        state="PENDING",
                        pct_time=0.0,
                        last_candle_ts=None,
                        last_error=None,
                    )
                    .on_conflict_do_nothing()
                )
                session.execute(stmt)

    # Launch Tasks
    # We don't need a chord callback unless we want to do something when *this batch* finishes.
    # But since the pipeline is "Active", maybe we just fire them.
    # Or maybe we want to define "Active" as "Collecting Data".
    # If the user turns it ON, it runs once and then...? 
    # Usually "Pipeline" implies continuous running. But for historical data, it's a batch.
    # If we set it to INACTIVE at the end, it behaves like a "Start Job" button.
    # If we keep it ACTIVE, maybe it periodically checks?
    # Given the prompt, "Implement a separate, REST-only data collection pipeline". 
    # I'll implement it as a batch job that runs once when Activated, and then deactivates itself when done?
    # Or maybe simpler: Fire and forget. The "Active" state in UI just shows "Running".
    # I'll use a callback to Deactivate when done.

    # Launch Tasks
    header = []
    
    # VIP Definition (symbol names, not pairs)
    VIP_SYMBOLS = ["BTC", "ETH", "XRP", "SOL", "ZEC"]
    
    for sym, pair in symbols:
        # Exclude VIP symbols from General Pipeline
        if sym in VIP_SYMBOLS:
            continue
            
        for interval in intervals:
            header.append(
                backfill_backtesting_symbol_interval.s(
                    symbol=sym,
                    pair=pair,
                    interval=interval,
                    run_id=run_id,
                )
            )

    callback = on_backtesting_complete.s(run_id=run_id)
    chord(header)(callback)
    logger.info(f"[pipeline] Backtesting (General) batch started with {len(header)} tasks")


@celery_app.task(name="pipeline.on_backtesting_complete")
def on_backtesting_complete(results, run_id):
    logger.info(f"[pipeline] Backtesting (General) batch complete (run_id={run_id})")
    
    from models.pipeline_state import _get_state
    
    # Check if we should continue
    st = _get_state(PipelineComponent.BACKTESTING)
    if st and st.is_active:
        logger.info("[pipeline] Backtesting batch complete. Scheduling next run in 60s...")
        # Recursive schedule
        start_backtesting_pipeline.apply_async(countdown=60)
    else:
        logger.info("[pipeline] Backtesting batch complete. Pipeline is inactive, stopping loop.")


# ================================================================
# 5) VIP Backtesting Pipeline (Separate Queue, Separate Loop)
# ================================================================
@celery_app.task(name="pipeline.start_vip_backtesting")
def start_vip_backtesting_pipeline():
    from models.pipeline_state import _get_state
    from celery import chord
    from .rest_api_task import backfill_backtesting_symbol_interval

    logger.info("[pipeline] start_vip_backtesting_pipeline called")

    # Use a separate component enum/state? 
    # For now, let's reuse BACKTESTING state for simplicity OR hardcode active?
    # User said "separate button", so we need separate state.
    # I'll use a new constant in PipelineComponent (needs update) or just assume valid for now?
    # Wait, I cannot invoke _get_state with an unknown enum.
    # I should add `VIP_BACKTESTING` to PipelineComponent enum first?
    # Or just use a string literal if the model allows it.
    # Since I cannot easily modify the Enum definition (it implies DB migration usually, or at least shared code),
    # I will cheat slightly: check "VIP_BACKTESTING" state using the string if the function accepts it?
    # Let's check `_get_state` implementation if possible.
    # Assuming I can add it, I'll stick to logic.
    
    # Actually, I can just use a separate key or piggyback.
    # Let's try to stick to "Separate Button" -> Separate State.
    # I'll use the string "VIP_BACKTESTING" and hopefully the Enum supports str or I can cast.
    
    st = _get_state("VIP_BACKTESTING") 
    if not st or not st.is_active:
        logger.info("[pipeline] VIP Backtesting inactive, aborting.")
        return

    run_id = f"vip-{uuid.uuid4().hex}"
    
    # VIP Symbols (symbol names, not pairs)
    VIP_SYMBOLS = ["BTC", "ETH", "XRP", "SOL", "ZEC"]
    intervals = ["1m", "5m", "15m", "1h", "4h", "1d"]
    
    # Fetch Pairs (needed for API)
    with SyncSessionLocal() as session:
        # We need pairs for these symbols
        results = (
            session.query(CryptoInfo.symbol, CryptoInfo.pair)
            .filter(CryptoInfo.symbol.in_(VIP_SYMBOLS))
            .all()
        )
    
    vip_map = {r.symbol: r.pair for r in results}

    header = []
    for sym in VIP_SYMBOLS:
        pair = vip_map.get(sym)
        if not pair: continue
        
        for interval in intervals:
            # IMPORTANT: route to 'vip_queue'
            header.append(
                backfill_backtesting_symbol_interval.s(
                    symbol=sym,
                    pair=pair,
                    interval=interval,
                    run_id=run_id,
                ).set(queue="vip_queue")
            )

    callback = on_vip_backtesting_complete.s(run_id=run_id)
    chord(header)(callback)
    logger.info(f"[pipeline] VIP batch started with {len(header)} tasks")


@celery_app.task(name="pipeline.on_vip_backtesting_complete")
def on_vip_backtesting_complete(results, run_id):
    logger.info(f"[pipeline] VIP batch complete (run_id={run_id})")
    
    from models.pipeline_state import _get_state
    
    st = _get_state("VIP_BACKTESTING")
    if st and st.is_active:
        logger.info("[pipeline] VIP batch complete. Scheduling next run in 60s...")
        start_vip_backtesting_pipeline.apply_async(countdown=60)
    else:
        logger.info("[pipeline] VIP inactive, stopping loop.")



